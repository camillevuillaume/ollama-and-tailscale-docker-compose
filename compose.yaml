services:
  tailscale-ollama:
    image: tailscale/tailscale:latest
    hostname: tailscale-ollama
    environment:
      - TS_AUTHKEY=${TS_AUTH_KEY}
      - TS_EXTRA_ARGS=--advertise-tags=tag:container
      - TS_STATE_DIR=/var/lib/tailscale
      - TS_USERSPACE=false
    volumes:
      - ${PWD}/tailscale-nginx/state:/var/lib/tailscale
    devices:
      - /dev/net/tun:/dev/net/tun
    cap_add:
      - net_admin
    restart: unless-stopped
  ollama:
    #AMD GPU SUPPORT
    image: ollama/ollama:rocm
    devices:
      - /dev/dri
      - /dev/kfd
It has been tested locally on a Proxmox VM without GPU acceleration. Next I will try it out on EC2 instances with GPUs. My goal is to integrate it in some of my local workflows, for instance with N8N to accelerate LLMs.
   # NVIDIA GPU SUPPORT
   # image: ollama/ollama:latest
   # deploy:
   #   resources:
   #     reservations:
   #       devices:
   #         - driver: nvidia
   #           count: all
   #           capabilities: [gpu]
    depends_on:
      - tailscale-ollama
    network_mode: service:tailscale-ollama
