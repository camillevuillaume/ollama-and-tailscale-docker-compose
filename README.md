# ollama-and-tailscale-docker-compose
Docker compose yaml for Ollama and Tailscale.
It has been tested locally on a Proxmox VM without GPU acceleration. Next I will try it out on EC2 instances with GPUs. My goal is to integrate it in some of my local workflows, for instance with N8N to accelerate LLMs.
